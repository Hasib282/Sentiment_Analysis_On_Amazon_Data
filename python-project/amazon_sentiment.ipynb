{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4574, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/amazon_datasets.csv')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# General Preprocessing (normalization, spaces)\n",
    "def basic_preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower() # Convert text to lowercase\n",
    "    text = re.sub(r'\\d+', '', text) # Remove numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "# VADER Preprocessing\n",
    "def preprocess_vader(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    return basic_preprocess(text)\n",
    "\n",
    "\n",
    "# Transformer Models Preprocessing (keep meaningful punctuation)\n",
    "def preprocess_transformers(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = basic_preprocess(text)\n",
    "    text = re.sub(r'[^\\w\\s,!?]', '', text) # Remove punctuation except meaningful ones\n",
    "    return text\n",
    "\n",
    "\n",
    "# Deep Learning Models (CNN, LSTM) Preprocessing (Remove punctuation)\n",
    "def preprocess_dl_models(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = basic_preprocess(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    return text\n",
    "\n",
    "\n",
    "# SVM Preprocessing (Remove punctuation, and stop words)\n",
    "def preprocess_svm(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = basic_preprocess(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words]) # Remove stop words\n",
    "    return text\n",
    "\n",
    "\n",
    "# Assign Sentiment on the basis of customer ratings\n",
    "def assign_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 1\n",
    "    elif rating == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # General Preprocessing (normalization, spaces)\n",
    "# def basic_preprocess(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.lower()  # Convert text to lowercase\n",
    "#     text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "#     return text\n",
    "\n",
    "# # Unified Preprocessing (for all models)\n",
    "# def preprocess_data(text):\n",
    "#     text = basic_preprocess(text)\n",
    "#     text = re.sub(r'[^\\w\\s,!?]', '', text)  # Remove punctuation except meaningful ones\n",
    "#     return text\n",
    "\n",
    "# # Assign Sentiment on the basis of customer ratings\n",
    "# def assign_sentiment(rating):\n",
    "#     if rating >= 4:\n",
    "#         return 1\n",
    "#     elif rating == 3:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applyin Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>product_title</th>\n",
       "      <th>user_name</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>review_date</th>\n",
       "      <th>vader</th>\n",
       "      <th>transformers</th>\n",
       "      <th>cnn_lstm</th>\n",
       "      <th>svm</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>OnePlus Nord N30</td>\n",
       "      <td>forest</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this phone at the recommendation of a...</td>\n",
       "      <td>May 19, 2024</td>\n",
       "      <td>i bought this phone at the recommendation of a...</td>\n",
       "      <td>i bought this phone at the recommendation of a...</td>\n",
       "      <td>i bought this phone at the recommendation of a...</td>\n",
       "      <td>bought phone recommendation friend happy im so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>OnePlus Nord N30</td>\n",
       "      <td>Drew</td>\n",
       "      <td>5</td>\n",
       "      <td>I have this phone for a few months now and for...</td>\n",
       "      <td>July 26, 2024</td>\n",
       "      <td>i have this phone for a few months now and for...</td>\n",
       "      <td>i have this phone for a few months now and for...</td>\n",
       "      <td>i have this phone for a few months now and for...</td>\n",
       "      <td>phone months price great phone looking somethi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>OnePlus Nord N30</td>\n",
       "      <td>forest</td>\n",
       "      <td>4</td>\n",
       "      <td>I like that this phone has a good battery life...</td>\n",
       "      <td>January 2, 2024</td>\n",
       "      <td>i like that this phone has a good battery life...</td>\n",
       "      <td>i like that this phone has a good battery life...</td>\n",
       "      <td>i like that this phone has a good battery life...</td>\n",
       "      <td>like phone good battery life charges superfast...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>OnePlus Nord N30</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>5</td>\n",
       "      <td>Short version: I got this for my mom since she...</td>\n",
       "      <td>August 10, 2024</td>\n",
       "      <td>short version: i got this for my mom since she...</td>\n",
       "      <td>short version i got this for my mom since she ...</td>\n",
       "      <td>short version i got this for my mom since she ...</td>\n",
       "      <td>short version got mom since habit buying cheap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>OnePlus Nord N30</td>\n",
       "      <td>C Jack</td>\n",
       "      <td>5</td>\n",
       "      <td>I have a Samsung s22 Ultra. I've been having b...</td>\n",
       "      <td>May 27, 2024</td>\n",
       "      <td>i have a samsung s ultra. i've been having bat...</td>\n",
       "      <td>i have a samsung s ultra ive been having batte...</td>\n",
       "      <td>i have a samsung s ultra ive been having batte...</td>\n",
       "      <td>samsung ultra ive battery connectivity issues ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     product_title        user_name  rating  \\\n",
       "0   1  OnePlus Nord N30           forest       5   \n",
       "1   2  OnePlus Nord N30             Drew       5   \n",
       "2   3  OnePlus Nord N30           forest       4   \n",
       "3   4  OnePlus Nord N30  Amazon Customer       5   \n",
       "4   5  OnePlus Nord N30           C Jack       5   \n",
       "\n",
       "                                              review      review_date  \\\n",
       "0  I bought this phone at the recommendation of a...     May 19, 2024   \n",
       "1  I have this phone for a few months now and for...    July 26, 2024   \n",
       "2  I like that this phone has a good battery life...  January 2, 2024   \n",
       "3  Short version: I got this for my mom since she...  August 10, 2024   \n",
       "4  I have a Samsung s22 Ultra. I've been having b...     May 27, 2024   \n",
       "\n",
       "                                               vader  \\\n",
       "0  i bought this phone at the recommendation of a...   \n",
       "1  i have this phone for a few months now and for...   \n",
       "2  i like that this phone has a good battery life...   \n",
       "3  short version: i got this for my mom since she...   \n",
       "4  i have a samsung s ultra. i've been having bat...   \n",
       "\n",
       "                                        transformers  \\\n",
       "0  i bought this phone at the recommendation of a...   \n",
       "1  i have this phone for a few months now and for...   \n",
       "2  i like that this phone has a good battery life...   \n",
       "3  short version i got this for my mom since she ...   \n",
       "4  i have a samsung s ultra ive been having batte...   \n",
       "\n",
       "                                            cnn_lstm  \\\n",
       "0  i bought this phone at the recommendation of a...   \n",
       "1  i have this phone for a few months now and for...   \n",
       "2  i like that this phone has a good battery life...   \n",
       "3  short version i got this for my mom since she ...   \n",
       "4  i have a samsung s ultra ive been having batte...   \n",
       "\n",
       "                                                 svm  sentiment  \n",
       "0  bought phone recommendation friend happy im so...          1  \n",
       "1  phone months price great phone looking somethi...          1  \n",
       "2  like phone good battery life charges superfast...          1  \n",
       "3  short version got mom since habit buying cheap...          1  \n",
       "4  samsung ultra ive battery connectivity issues ...          1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vader']         =   df['review'].apply(preprocess_vader)\n",
    "df['transformers']  =   df['review'].apply(preprocess_transformers)\n",
    "df['cnn_lstm']      =   df['review'].apply(preprocess_dl_models)\n",
    "df['svm']           =   df['review'].apply(preprocess_svm)\n",
    "df['sentiment']     =   df['rating'].apply(assign_sentiment)\n",
    "\n",
    "df.to_csv('clean_datasets.csv', index=False)\n",
    "\n",
    "# df = df.head(500)\n",
    "df.head()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['clean_review']  =   df['review'].apply(preprocess_data)\n",
    "# df['sentiment']     =   df['rating'].apply(assign_sentiment)\n",
    "\n",
    "# df.to_csv('clean_datasets.csv', index=False)\n",
    "\n",
    "# df = df.head(500)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data Into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_text_train, X_text_test, X_vader_train, X_vader_test, X_transformers_train, X_transformers_test, X_cnn_lstm_train, X_cnn_lstm_test, X_svm_train, X_svm_test, y_train, y_test = train_test_split(\n",
    "    df['review'], df['vader'], df['transformers'], df['cnn_lstm'], df['svm'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = df['clean_review']\n",
    "# Y = df['sentiment']\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Initializing Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer-Based Models: ( RoBERTa, BERT )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import RobertaForSequenceClassification, RobertaTokenizer, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# def preprocess_transformers(texts, tokenizer, max_length=128):\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "#     return inputs\n",
    "\n",
    "# # Load Pre-Train Models\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Roberta Model\n",
    "# def roberta_sentiment(text):\n",
    "#     roberta_inputs = preprocess_transformers([text], roberta_tokenizer)\n",
    "#     roberta_inputs = {key: val.to(device) for key, val in roberta_inputs.items()}  # Move inputs to device\n",
    "#     with torch.no_grad():\n",
    "#         roberta_output = roberta_model(**roberta_inputs).logits\n",
    "#     probabilities = F.softmax(roberta_output, dim=-1)\n",
    "#     positive_class_prob = probabilities[0, 1].item()\n",
    "#     return positive_class_prob\n",
    "\n",
    "\n",
    "# # Bert Model\n",
    "# def bert_sentiment(text):\n",
    "#     bert_inputs = preprocess_transformers([text], bert_tokenizer)\n",
    "#     bert_inputs = {key: val.to(device) for key, val in bert_inputs.items()}  # Move inputs to device\n",
    "#     with torch.no_grad():\n",
    "#         bert_output = bert_model(**bert_inputs).logits\n",
    "#     probabilities = F.softmax(bert_output, dim=-1)\n",
    "#     positive_class_prob = probabilities[0, 1].item()\n",
    "#     return positive_class_prob\n",
    "\n",
    "\n",
    "# # Roberta Model\n",
    "# def roberta_sentiment(text):\n",
    "#     roberta_inputs = preprocess_transformers([text], roberta_tokenizer)\n",
    "#     roberta_inputs = {key: val.to(device) for key, val in roberta_inputs.items()}  \n",
    "#     with torch.no_grad():\n",
    "#         roberta_output = roberta_model(**roberta_inputs).logits\n",
    "#     probabilities = F.softmax(roberta_output, dim=-1)\n",
    "#     positive_class_prob = probabilities[0, 1].item()\n",
    "#     negative_class_prob = probabilities[0, 0].item()\n",
    "#     neutral_class_prob = probabilities[0, 2].item()\n",
    "#     compound_score = (positive_class_prob - negative_class_prob) * (1 - neutral_class_prob)\n",
    "#     return compound_score\n",
    "\n",
    "# # Bert Model\n",
    "# def bert_sentiment(text):\n",
    "#     bert_inputs = preprocess_transformers([text], bert_tokenizer)\n",
    "#     bert_inputs = {key: val.to(device) for key, val in bert_inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         bert_output = bert_model(**bert_inputs).logits\n",
    "#     probabilities = F.softmax(bert_output, dim=-1)\n",
    "#     positive_class_prob = probabilities[0, 1].item()\n",
    "#     negative_class_prob = probabilities[0, 0].item()\n",
    "#     neutral_class_prob = probabilities[0, 2].item()\n",
    "#     compound_score = (positive_class_prob - negative_class_prob) * (1 - neutral_class_prob)\n",
    "#     return compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasib282\\anaconda\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "# from transformers import RobertaForSequenceClassification, BertForSequenceClassification, AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def preprocess_transformers(texts, tokenizer, max_length=128):\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    return inputs\n",
    "\n",
    "# Load Pre-Train Models\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment')\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "bert_model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "\n",
    "# Roberta Model\n",
    "def roberta_sentiment(text):\n",
    "    roberta_inputs = preprocess_transformers([text], roberta_tokenizer)\n",
    "    roberta_inputs = {key: val.to(device) for key, val in roberta_inputs.items()}  \n",
    "    with torch.no_grad():\n",
    "        roberta_output = roberta_model(**roberta_inputs).logits\n",
    "    probabilities = F.softmax(roberta_output, dim=-1)\n",
    "    positive_class_prob = probabilities[0, 1].item()\n",
    "    negative_class_prob = probabilities[0, 0].item()\n",
    "    neutral_class_prob = probabilities[0, 2].item()\n",
    "    return positive_class_prob - negative_class_prob\n",
    "    # compound_score = positive_class_prob - negative_class_prob\n",
    "    # compound_score = (positive_class_prob - negative_class_prob) * (1 - neutral_class_prob)\n",
    "    # return compound_score\n",
    "\n",
    "\n",
    "def bert_sentiment(text):\n",
    "    bert_inputs = preprocess_transformers([text], bert_tokenizer)\n",
    "    bert_inputs = {key: val.to(device) for key, val in bert_inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**bert_inputs).logits\n",
    "    probabilities = F.softmax(bert_output, dim=-1)\n",
    "    positive_class_prob = probabilities[0, 1].item()\n",
    "    negative_class_prob = probabilities[0, 0].item()\n",
    "    neutral_class_prob = probabilities[0, 2].item()\n",
    "    return positive_class_prob - negative_class_prob\n",
    "    # compound_score = positive_class_prob - negative_class_prob\n",
    "    # compound_score = (positive_class_prob - negative_class_prob) * (1 - neutral_class_prob)\n",
    "    # return compound_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# # Load Pre-Trained Models\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Function to preprocess data for transformer models (RoBERTa, BERT, GPT-2)\n",
    "# def preprocess_transformers(texts, tokenizer, max_length=128):\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "#     return inputs\n",
    "\n",
    "# # Function to run a sentiment model\n",
    "# def roberta_sentiment(text):\n",
    "#     inputs = preprocess_transformers([text], roberta_tokenizer)\n",
    "#     outputs = roberta_model(**inputs).logits\n",
    "#     probabilities = F.softmax(outputs, dim=-1)\n",
    "#     positive_prob = probabilities[0, 1].item()\n",
    "#     return positive_prob\n",
    "\n",
    "# def bert_sentiment(text):\n",
    "#     inputs = preprocess_transformers([text], bert_tokenizer)\n",
    "#     outputs = bert_model(**inputs).logits\n",
    "#     probabilities = F.softmax(outputs, dim=-1)\n",
    "#     positive_prob = probabilities[0, 1].item()\n",
    "#     return positive_prob\n",
    "\n",
    "\n",
    "# def gpt2_sentiment(text):\n",
    "#     inputs = preprocess_transformers([text], gpt2_tokenizer)\n",
    "#     outputs = gpt2_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "#     positive_prob = torch.sigmoid(outputs).mean().item()\n",
    "#     return positive_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexicon-Based Approaches: ( VADER, TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Vader Model \n",
    "def vader_sentiment(text):\n",
    "    return vader_analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "\n",
    "# TextBlob Model \n",
    "def textblob_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    positive_prob = (polarity + 1) / 2  # Normalize to [0, 1]\n",
    "    negative_prob = 1 - positive_prob\n",
    "    return positive_prob - negative_prob\n",
    "    # return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# def textblob_sentiment(text):\n",
    "#     polarity = TextBlob(text).sentiment.polarity\n",
    "    \n",
    "#     neutral_prob = 1 - abs(polarity)\n",
    "    \n",
    "#     positive_prob = (polarity + 1) / 2  # Convert [-1, 1] polarity to [0, 1]\n",
    "#     negative_prob = 1 - positive_prob\n",
    "#     compound_score = (positive_prob - negative_prob) * (1 - neutral_prob)\n",
    "#     print('textblob', positive_prob, negative_prob, neutral_prob, compound_score )\n",
    "#     return compound_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning Model: SVM with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# # TF-IDF Method\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_svm_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_svm_test)\n",
    "\n",
    "# def tfidf_features(texts):\n",
    "#     return tfidf_vectorizer.transform(texts).toarray()\n",
    "\n",
    "\n",
    "# # SVM Model with TF-IDF features\n",
    "# svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(probability=True))\n",
    "# svm_model.fit(X_train_tfidf, y_train)\n",
    "# svm_predictions = svm_model.predict_proba(X_test_tfidf)[:, 1]  # Probability of positive class\n",
    "\n",
    "# def svm_score(tfidf_features):\n",
    "#     svm_model.predict_proba(tfidf_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# import numpy as np\n",
    "\n",
    "# SVM Model definition using TF-IDF Vectorizer\n",
    "svm_model = make_pipeline(TfidfVectorizer(max_features=5000), SVC(probability=True))\n",
    "\n",
    "# Training the SVM model\n",
    "svm_model.fit(X_svm_train, y_train)\n",
    "\n",
    "# SVM Sentiment Prediction function\n",
    "def svm_sentiment(text):\n",
    "    probabilities = svm_model.predict_proba([text])[0]\n",
    "    \n",
    "    negative_prob = probabilities[0]\n",
    "    neutral_prob = probabilities[1]\n",
    "    positive_prob = probabilities[2]\n",
    "    \n",
    "    \n",
    "    # compound_score = (positive_prob - negative_prob) * (1 - neutral_prob)\n",
    "    # print('svm', positive_prob, negative_prob, neutral_prob, compound_score )\n",
    "    # compound_score = positive_prob - negative_prob\n",
    "    return positive_prob - negative_prob\n",
    "    # return compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# # TF-IDF Method\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# def tfidf_features(texts):\n",
    "#     return tfidf_vectorizer.transform(texts).toarray()\n",
    "\n",
    "\n",
    "# # SVM Model with TF-IDF features\n",
    "# svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(probability=True))\n",
    "# svm_model.fit(X_train_tfidf, Y_train)\n",
    "# svm_predictions = svm_model.predict_proba(X_test_tfidf)[:, 1]  # Probability of positive class\n",
    "\n",
    "# def svm_score(tfidf_features):\n",
    "#     svm_model.predict_proba(tfidf_features)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# # Function to preprocess for SVM\n",
    "# def preprocess_svm(texts, vectorizer):\n",
    "#     return vectorizer.transform(texts)\n",
    "\n",
    "# # Train SVM model with TF-IDF vectorizer\n",
    "# def train_svm(x_data, y_data):\n",
    "#     vectorizer = TfidfVectorizer(max_features=2000)\n",
    "#     X_train_tfidf = vectorizer.fit_transform(x_data)\n",
    "#     svm = SVC(probability=True)\n",
    "#     svm.fit(X_train_tfidf, y_data)\n",
    "#     return svm, vectorizer\n",
    "\n",
    "# def svm_sentiment(text, vectorizer, model):\n",
    "#     X = preprocess_svm([text], vectorizer)\n",
    "#     return model.predict_proba(X)[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning Models: (LSTM, CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "\n",
    "# # lstm Model \n",
    "# def build_lstm(vocab_size, embedding_dim=128, max_length=100):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "#     model.add(LSTM(128, return_sequences=False))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # CNN Model\n",
    "# def build_cnn(vocab_size, embedding_dim=128, max_length=100):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "#     model.add(Conv1D(128, 5, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=2))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Create Tokenizer for LSTM and CNN\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(X_cnn_lstm_train)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_cnn_lstm_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_cnn_lstm_test)\n",
    "\n",
    "# max_length = 100\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# # Build and train LSTM and CNN models\n",
    "# lstm_model = build_lstm(vocab_size, max_length=max_length)\n",
    "# cnn_model = build_cnn(vocab_size, max_length=max_length)\n",
    "\n",
    "# lstm_model.fit(X_train_pad, np.array(y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "# cnn_model.fit(X_train_pad, np.array(y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# # LSTM and CNN Predictions\n",
    "# lstm_predictions = lstm_model.predict(X_test_pad).flatten()\n",
    "# cnn_predictions = cnn_model.predict(X_test_pad).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hasib282\\anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 328ms/step - accuracy: 0.8245 - loss: 0.6415 - val_accuracy: 0.7750 - val_loss: 0.6546\n",
      "Epoch 2/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step - accuracy: 0.8079 - loss: 0.6138 - val_accuracy: 0.7750 - val_loss: 0.6164\n",
      "Epoch 3/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 146ms/step - accuracy: 0.8264 - loss: 0.5660 - val_accuracy: 0.7750 - val_loss: 0.6527\n",
      "Epoch 4/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.8283 - loss: 0.5211 - val_accuracy: 0.7750 - val_loss: 0.6239\n",
      "Epoch 5/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - accuracy: 0.8159 - loss: 0.5208 - val_accuracy: 0.7750 - val_loss: 0.6079\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Parameters\n",
    "vocab_size = 10000  # Vocabulary size\n",
    "max_length = 100  # Max length for input sequences\n",
    "embedding_dim = 100  # Embedding vector size\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_cnn_lstm_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_cnn_lstm_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_cnn_lstm_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "\n",
    "# lstm Model \n",
    "def build_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create and train the LSTM model\n",
    "lstm_model = build_lstm()\n",
    "lstm_model.fit(X_train_pad, np.array(y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "# lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# LSTM Sentiment Prediction\n",
    "def lstm_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    positive_prob = lstm_model.predict(padded_sequence)[0][0]\n",
    "    negative_prob = 1 - positive_prob\n",
    "    return positive_prob - negative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 131ms/step - accuracy: 0.7506 - loss: 0.6453 - val_accuracy: 0.7750 - val_loss: 0.6875\n",
      "Epoch 2/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8048 - loss: 0.5978 - val_accuracy: 0.7750 - val_loss: 0.6461\n",
      "Epoch 3/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8153 - loss: 0.4772 - val_accuracy: 0.7750 - val_loss: 0.6332\n",
      "Epoch 4/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8333 - loss: 0.3719 - val_accuracy: 0.7750 - val_loss: 0.6439\n",
      "Epoch 5/5\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.8595 - loss: 0.3368 - val_accuracy: 0.7750 - val_loss: 0.6126\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "\n",
    "# CNN Model\n",
    "def build_cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(Conv1D(128, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the CNN model\n",
    "cnn_model = build_cnn()\n",
    "cnn_model.fit(X_train_pad, np.array(y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Function for predicting sentiment using CNN\n",
    "def cnn_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "    positive_prob = cnn_model.predict(padded_sequence)[0][0]\n",
    "    negative_prob = 1 - positive_prob\n",
    "    return positive_prob - negative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "\n",
    "\n",
    "# # LSTM Model\n",
    "# def Build_lstm_model(input_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(MAX_NUM_WORDS, 128, input_length=input_length))\n",
    "#     model.add(LSTM(128, return_sequences=False))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def Build_cnn_model(input_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=5000, output_dim=128, input_length=input_length))\n",
    "#     model.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=4))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Tokenizer for LSTM/CNN\n",
    "# MAX_NUM_WORDS = 20000\n",
    "# MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "# # Create Tokenizer for LSTM and CNN\n",
    "# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# # LSTM Model training\n",
    "# lstm_model = Build_lstm_model(input_length=MAX_SEQUENCE_LENGTH)\n",
    "# lstm_model.fit(X_train_pad, np.array(Y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# # CNN Model training\n",
    "# cnn_model = Build_cnn_model(input_length=MAX_SEQUENCE_LENGTH)\n",
    "# cnn_model.fit(X_train_pad, np.array(Y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# # LSTM and CNN Predictions\n",
    "# lstm_predictions = lstm_model.predict(X_test_pad).flatten()\n",
    "# cnn_predictions = cnn_model.predict(X_test_pad).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# def hybrid_model(texts, vader_texts, transformers_texts, svm_texts):\n",
    "#     results = []\n",
    "    \n",
    "#     for i, text in enumerate(texts):\n",
    "#         # Vader Sentiment\n",
    "#         vader_score = vader_sentiment(vader_texts[i])\n",
    "        \n",
    "#         # Transformer Models (RoBERTa, BERT)\n",
    "#         roberta_score = roberta_sentiment(transformers_texts[i])\n",
    "#         bert_score = bert_sentiment(transformers_texts[i])\n",
    "\n",
    "#         # SVM Prediction (using TF-IDF)\n",
    "#         svm_score = svm_sentiment(svm_texts[i])\n",
    "\n",
    "#         # Additional Models: TextBlob\n",
    "#         textblob_score = textblob_sentiment(text)\n",
    "\n",
    "#         # LSTM and CNN Predictions\n",
    "#         lstm_score = lstm_predictions[i]\n",
    "#         cnn_score = cnn_predictions[i]\n",
    "\n",
    "#         # SVM Prediction (using TF-IDF)\n",
    "#         # tfidf_features = tfidf_vectorizer.transform([svm_texts[i]])\n",
    "#         # svm_score = svm_model.predict(tfidf_features)[0]\n",
    "        \n",
    "\n",
    "#         # POS Tags (custom function for counting nouns and verbs)\n",
    "#         noun_count, verb_count = pos_tags(text)\n",
    "\n",
    "#         # Calculate average score (simple hybrid result aggregation)\n",
    "#         avg_score = np.mean([\n",
    "#             vader_score, textblob_score,  roberta_score, bert_score, lstm_score, cnn_score, svm_score\n",
    "#         ])\n",
    "\n",
    "#         # VADER\n",
    "#         if vader_score > 0.1:\n",
    "#             sentiment_vader = \"Positive\"\n",
    "#         elif vader_score == -0.1:\n",
    "#             sentiment_vader = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_vader = \"Negative\"\n",
    "\n",
    "#         # TextBlob\n",
    "#         if textblob_score > 0.1:\n",
    "#             sentiment_textblob = \"Positive\"\n",
    "#         elif textblob_score == -0.1:\n",
    "#             sentiment_textblob = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_textblob = \"Negative\"\n",
    "\n",
    "#         # RoBERTa\n",
    "#         if roberta_score > 0.1:\n",
    "#             sentiment_roberta = \"Positive\"\n",
    "#         elif roberta_score < -0.1:\n",
    "#             sentiment_roberta = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_roberta = \"Neutral\"\n",
    "\n",
    "#         # BERT\n",
    "#         if bert_score > 0.1:\n",
    "#             sentiment_bert = \"Positive\"\n",
    "#         elif bert_score < -0.1:\n",
    "#             sentiment_bert = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_bert = \"Neutral\"\n",
    "\n",
    "#         # CNN\n",
    "#         if cnn_score > 0.55:\n",
    "#             sentiment_cnn = \"Positive\"\n",
    "#         elif cnn_score < 0.45:\n",
    "#             sentiment_cnn = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_cnn = \"Neutral\"\n",
    "\n",
    "#         # LSTM\n",
    "#         if lstm_score > 0.55:\n",
    "#             sentiment_lstm = \"Positive\"\n",
    "#         elif lstm_score < 0.45:\n",
    "#             sentiment_lstm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_lstm = \"Neutral\"\n",
    "        \n",
    "#         # SVM\n",
    "#         if svm_score == 1:\n",
    "#             sentiment_svm = \"Positive\"\n",
    "#         elif svm_score == 0:\n",
    "#             sentiment_svm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_svm = \"Neutral\"\n",
    "\n",
    "#         # Average score\n",
    "#         if avg_score > 0.55:\n",
    "#             sentiment_avg = \"Positive\"\n",
    "#         elif avg_score < 0.45:\n",
    "#             sentiment_avg = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_avg = \"Neutral\"\n",
    "\n",
    "#         # Append results for this text\n",
    "#         results.append({\n",
    "#             'text': text,\n",
    "#             'vader': vader_score,\n",
    "#             'textblob': textblob_score,\n",
    "#             'roberta': roberta_score,\n",
    "#             'bert': bert_score,\n",
    "#             'cnn': cnn_score,\n",
    "#             'lstm': lstm_score,\n",
    "#             'svm': svm_score,\n",
    "#             'average_score': avg_score,\n",
    "#             'sentiment': df['sentiment'][i],\n",
    "#             'vader_sentiment': sentiment_vader,\n",
    "#             'textblob_sentiment': sentiment_textblob,\n",
    "#             'roberta_sentiment': sentiment_roberta,\n",
    "#             'bert_sentiment': sentiment_bert,\n",
    "#             'cnn_sentiment': sentiment_cnn,\n",
    "#             'lstm_sentiment': sentiment_lstm,\n",
    "#             'svm_sentiment': sentiment_svm,\n",
    "#             'avg_sentiment': sentiment_avg,\n",
    "#             'nouncount_': noun_count,\n",
    "#             'verbcount_': verb_count\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# df['review'] = df['review'].fillna('').astype(str)\n",
    "# min_length = min(len(X_text_test), len(X_vader_test), len(X_transformers_test), len(X_cnn_lstm_test), len(X_svm_test))\n",
    "\n",
    "# texts = X_text_test[:min_length].reset_index(drop=True)\n",
    "# vader_texts = X_vader_test[:min_length].reset_index(drop=True)\n",
    "# transformers_texts = X_transformers_test[:min_length].reset_index(drop=True)\n",
    "# cnn_lstm_texts = X_cnn_lstm_test[:min_length].reset_index(drop=True)\n",
    "# svm_texts = X_svm_test[:min_length].reset_index(drop=True)\n",
    "\n",
    "# # Run the hybrid model\n",
    "# hybrid_results = hybrid_model(texts, vader_texts, transformers_texts, svm_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "# def hybrid_model(texts, vader_texts, transformers_texts, svm_texts):\n",
    "#     results = []\n",
    "    \n",
    "#     for i, text in enumerate(texts):\n",
    "#         # Vader Sentiment\n",
    "#         vader_score = vader_sentiment(vader_texts[i])\n",
    "#         textblob_score = textblob_sentiment(text)\n",
    "\n",
    "#         # Transformer Models (RoBERTa, BERT)\n",
    "#         roberta_score = roberta_sentiment(transformers_texts[i])\n",
    "#         bert_score = bert_sentiment(transformers_texts[i])\n",
    "#         gpt2_score = gpt2_sentiment(transformers_texts[i])\n",
    "\n",
    "#         # LSTM and CNN Predictions\n",
    "#         lstm_score = lstm_predictions[i]\n",
    "#         cnn_score = cnn_predictions[i]\n",
    "\n",
    "#         # SVM Prediction (using TF-IDF)\n",
    "#         tfidf_features = tfidf_vectorizer.transform([svm_texts[i]])\n",
    "#         svm_score = svm_model.predict(tfidf_features)[0]\n",
    "\n",
    "\n",
    "\n",
    "#         # POS Tags (custom function for counting nouns and verbs)\n",
    "#         noun_count, verb_count = pos_tags(text)\n",
    "\n",
    "#         # Calculate average score (simple hybrid result aggregation)\n",
    "#         avg_score = np.mean([\n",
    "#             vader_score, textblob_score,  roberta_score, bert_score, lstm_score, cnn_score, svm_score\n",
    "#         ])\n",
    "\n",
    "#         # VADER\n",
    "#         if vader_score > 0:\n",
    "#             sentiment_vader = \"Positive\"\n",
    "#         elif vader_score == 0:\n",
    "#             sentiment_vader = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_vader = \"Negative\"\n",
    "\n",
    "#         # TextBlob\n",
    "#         if textblob_score > 0:\n",
    "#             sentiment_textblob = \"Positive\"\n",
    "#         elif textblob_score == 0:\n",
    "#             sentiment_textblob = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_textblob = \"Negative\"\n",
    "\n",
    "#         # Gpt2\n",
    "#         if gpt2_score > 0.55:\n",
    "#             sentiment_gpt2 = \"Positive\"\n",
    "#         elif gpt2_score < 0.45:\n",
    "#             sentiment_gpt2 = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_gpt2 = \"Negative\"\n",
    "\n",
    "#         # RoBERTa\n",
    "#         if roberta_score > 0.55:\n",
    "#             sentiment_roberta = \"Positive\"\n",
    "#         elif roberta_score < 0.45:\n",
    "#             sentiment_roberta = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_roberta = \"Neutral\"\n",
    "\n",
    "#         # BERT\n",
    "#         if bert_score > 0.55:\n",
    "#             sentiment_bert = \"Positive\"\n",
    "#         elif bert_score < 0.45:\n",
    "#             sentiment_bert = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_bert = \"Neutral\"\n",
    "\n",
    "#         # CNN\n",
    "#         if cnn_score > 0.55:\n",
    "#             sentiment_cnn = \"Positive\"\n",
    "#         elif cnn_score < 0.45:\n",
    "#             sentiment_cnn = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_cnn = \"Neutral\"\n",
    "\n",
    "#         # LSTM\n",
    "#         if lstm_score > 0.55:\n",
    "#             sentiment_lstm = \"Positive\"\n",
    "#         elif lstm_score < 0.45:\n",
    "#             sentiment_lstm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_lstm = \"Neutral\"\n",
    "        \n",
    "#         # SVM\n",
    "#         if svm_score == 1:\n",
    "#             sentiment_svm = \"Positive\"\n",
    "#         elif svm_score == 0:\n",
    "#             sentiment_svm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_svm = \"Neutral\"\n",
    "\n",
    "#         # Average score\n",
    "#         if avg_score > 0.55:\n",
    "#             sentiment_avg = \"Positive\"\n",
    "#         elif avg_score < 0.45:\n",
    "#             sentiment_avg = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_avg = \"Neutral\"\n",
    "\n",
    "#         # Append results for this text\n",
    "#         results.append({\n",
    "#             'text': text,\n",
    "#             'vader': vader_score,\n",
    "#             'textblob': textblob_score,\n",
    "#             'roberta': roberta_score,\n",
    "#             'bert': bert_score,\n",
    "#             'gpt2': gpt2_score,\n",
    "#             'cnn': cnn_score,\n",
    "#             'lstm': lstm_score,\n",
    "#             'svm': svm_score,\n",
    "#             'average_score': avg_score,\n",
    "#             'sentiment': df['sentiment'][i],\n",
    "#             'vader_sentiment': sentiment_vader,\n",
    "#             'textblob_sentiment': sentiment_textblob,\n",
    "#             'roberta_sentiment': sentiment_roberta,\n",
    "#             'bert_sentiment': sentiment_bert,\n",
    "#             'gpt2_sentiment': sentiment_gpt2,\n",
    "#             'cnn_sentiment': sentiment_cnn,\n",
    "#             'lstm_sentiment': sentiment_lstm,\n",
    "#             'svm_sentiment': sentiment_svm,\n",
    "#             'avg_sentiment': sentiment_avg,\n",
    "#             'nouncount_': noun_count,\n",
    "#             'verbcount_': verb_count\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# df['review'] = df['review'].fillna('').astype(str)\n",
    "# min_length = min(len(X_text_test), len(X_vader_test), len(X_transformers_test), len(X_cnn_lstm_test), len(X_svm_test))\n",
    "\n",
    "# texts = X_text_test[:min_length].reset_index(drop=True)\n",
    "# vader_texts = X_vader_test[:min_length].reset_index(drop=True)\n",
    "# transformers_texts = X_transformers_test[:min_length].reset_index(drop=True)\n",
    "# cnn_lstm_texts = X_cnn_lstm_test[:min_length].reset_index(drop=True)\n",
    "# svm_texts = X_svm_test[:min_length].reset_index(drop=True)\n",
    "\n",
    "# # Run the hybrid model\n",
    "# hybrid_results = hybrid_model(texts, vader_texts, transformers_texts, svm_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_model(text, vader_text, transformers_text, cnn_lstm_text, svm_text):\n",
    "    vader_score = vader_sentiment(vader_text)\n",
    "    roberta_score = roberta_sentiment(transformers_text)\n",
    "    bert_score = bert_sentiment(transformers_text)\n",
    "    textblob_score = textblob_sentiment(text)\n",
    "    svm_score = svm_sentiment(svm_text)\n",
    "    cnn_score = cnn_sentiment(cnn_lstm_text)\n",
    "    lstm_score= lstm_sentiment(cnn_lstm_text)\n",
    "    \n",
    "    # Combine the predictions (averaging for simplicity)\n",
    "    hybrid_score = np.mean([vader_score, roberta_score, bert_score, textblob_score, svm_score])\n",
    "\n",
    "\n",
    "    # VADER\n",
    "    if vader_score > 0:\n",
    "        sentiment_vader = \"Positive\"\n",
    "    elif vader_score < 0:\n",
    "        sentiment_vader = \"Negative\"\n",
    "    else:\n",
    "        sentiment_vader = \"Neutral\"\n",
    "\n",
    "    # TextBlob\n",
    "    if textblob_score > 0:\n",
    "        sentiment_textblob = \"Positive\"\n",
    "    elif textblob_score < 0:\n",
    "        sentiment_textblob = \"Negative\"\n",
    "    else:\n",
    "        sentiment_textblob = \"Neutral\"\n",
    "\n",
    "    # RoBERTa\n",
    "    if roberta_score > 0:\n",
    "        sentiment_roberta = \"Positive\"\n",
    "    elif roberta_score < 0:\n",
    "        sentiment_roberta = \"Negative\"\n",
    "    else:\n",
    "        sentiment_roberta = \"Neutral\"\n",
    "\n",
    "    # BERT\n",
    "    if bert_score > 0:\n",
    "        sentiment_bert = \"Positive\"\n",
    "    elif bert_score < 0:\n",
    "        sentiment_bert = \"Negative\"\n",
    "    else:\n",
    "        sentiment_bert = \"Neutral\"\n",
    "\n",
    "    # CNN\n",
    "    if cnn_score > 0.55:\n",
    "        sentiment_cnn = \"Positive\"\n",
    "    elif cnn_score < 0.45:\n",
    "        sentiment_cnn = \"Negative\"\n",
    "    else:\n",
    "        sentiment_cnn = \"Neutral\"\n",
    "\n",
    "    # LSTM\n",
    "    if lstm_score > 0.55:\n",
    "        sentiment_lstm = \"Positive\"\n",
    "    elif lstm_score < 0.45:\n",
    "        sentiment_lstm = \"Negative\"\n",
    "    else:\n",
    "        sentiment_lstm = \"Neutral\"\n",
    "    \n",
    "    # SVM\n",
    "    if svm_score > 0:\n",
    "        sentiment_svm = \"Positive\"\n",
    "    elif svm_score < 0:\n",
    "        sentiment_svm = \"Negative\"\n",
    "    else:\n",
    "        sentiment_svm = \"Neutral\"\n",
    "\n",
    "    # Average score\n",
    "    if hybrid_score > 0.1:\n",
    "        sentiment_hybrid = \"Positive\"\n",
    "    elif hybrid_score < -0.1:\n",
    "        sentiment_hybrid = \"Negative\"\n",
    "    else:\n",
    "        sentiment_hybrid = \"Neutral\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comparison = {\n",
    "        \"text\": text,\n",
    "        \"vader_score\":  vader_score,\n",
    "        \"textblob_score\":  textblob_score,\n",
    "        \"roberta_score\":  roberta_score,\n",
    "        \"bert_score\":  bert_score,\n",
    "        \"cnn_score\":  cnn_score,\n",
    "        \"lstm_score\":  lstm_score,\n",
    "        \"svm_score\":  svm_score,\n",
    "        \"hybrid_score\":  hybrid_score,\n",
    "        'vader_sentiment': sentiment_vader,\n",
    "        'textblob_sentiment': sentiment_textblob,\n",
    "        'roberta_sentiment': sentiment_roberta,\n",
    "        'bert_sentiment': sentiment_bert,\n",
    "        'cnn_sentiment': sentiment_cnn,\n",
    "        'lstm_sentiment': sentiment_lstm,\n",
    "        'svm_sentiment': sentiment_svm,\n",
    "        'hybrid_sentiment': sentiment_hybrid,\n",
    "    }\n",
    "\n",
    "\n",
    "    return comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 103ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "Results saved to sentiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].fillna('').astype(str)\n",
    "min_length = min(len(X_text_test), len(X_vader_test), len(X_transformers_test), len(X_cnn_lstm_test), len(X_svm_test))\n",
    "\n",
    "texts = X_text_test[:min_length].reset_index(drop=True)\n",
    "vader_texts = X_vader_test[:min_length].reset_index(drop=True)\n",
    "transformers_texts = X_transformers_test[:min_length].reset_index(drop=True)\n",
    "cnn_lstm_texts = X_cnn_lstm_test[:min_length].reset_index(drop=True)\n",
    "svm_texts = X_svm_test[:min_length].reset_index(drop=True)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# Iterate over your dataset\n",
    "for i, text in enumerate(texts):\n",
    "    comparison = hybrid_model(text, vader_texts[i], transformers_texts[i], cnn_lstm_texts[i], svm_texts[i])\n",
    "    results_list.append(comparison)\n",
    "\n",
    "\n",
    "results_list = pd.DataFrame(results_list)\n",
    "results_list.to_csv('sentiment_results.csv', index=False)\n",
    "\n",
    "print(\"Results saved to sentiment_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare models sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_list = pd.DataFrame(results_list)\n",
    "# results_list.to_csv('sentiment_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.DataFrame(hybrid_results)\n",
    "\n",
    "# results_df.to_csv('amazon_sentiment_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('../dataset/amazon_datasets.csv')\n",
    "\n",
    "# # df = df.head(500)\n",
    "# df.shape\n",
    "\n",
    "# import re\n",
    "# import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# # nltk.download('stopwords')\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# # General Preprocessing (normalization, spaces)\n",
    "# def basic_preprocess(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.lower()  # Convert text to lowercase\n",
    "#     text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "#     return text\n",
    "\n",
    "# # Unified Preprocessing (for all models)\n",
    "# def preprocess_data(text):\n",
    "#     text = basic_preprocess(text)\n",
    "#     text = re.sub(r'[^\\w\\s,!?]', '', text)  # Remove punctuation except meaningful ones\n",
    "#     return text\n",
    "\n",
    "# # Assign Sentiment on the basis of customer ratings\n",
    "# def assign_sentiment(rating):\n",
    "#     if rating >= 4:\n",
    "#         return 1\n",
    "#     elif rating == 3:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return -1\n",
    "    \n",
    "\n",
    "# df['clean_review']  =   df['review'].apply(preprocess_data)\n",
    "# df['sentiment']     =   df['rating'].apply(assign_sentiment)\n",
    "\n",
    "# df.to_csv('clean_datasets.csv', index=False)\n",
    "\n",
    "# df = df.head(500)\n",
    "# df.shape\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_text = df['review']\n",
    "# X = df['clean_review']\n",
    "# Y = df['sentiment']\n",
    "\n",
    "# X_text_train, X_text_test, X_train, X_test, Y_train, Y_test = train_test_split(X_text, X, Y, test_size=0.2, random_state=32)\n",
    "\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification, BertTokenizer, BertForSequenceClassification, GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# # Load Pre-Trained Models\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "# roberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# gpt2_model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Function to preprocess data for transformer models (RoBERTa, BERT, GPT-2)\n",
    "# def preprocess_transformers(texts, tokenizer, max_length=128):\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "#     return inputs\n",
    "\n",
    "# # Function to run a sentiment model\n",
    "# def roberta_sentiment(text):\n",
    "#     inputs = preprocess_transformers([text], roberta_tokenizer)\n",
    "#     outputs = roberta_model(**inputs).logits\n",
    "#     probabilities = F.softmax(outputs, dim=-1)\n",
    "#     positive_prob = probabilities[0, 1].item()\n",
    "#     return positive_prob\n",
    "\n",
    "# def bert_sentiment(text):\n",
    "#     inputs = preprocess_transformers([text], bert_tokenizer)\n",
    "#     outputs = bert_model(**inputs).logits\n",
    "#     probabilities = F.softmax(outputs, dim=-1)\n",
    "#     positive_prob = probabilities[0, 1].item()\n",
    "#     return positive_prob\n",
    "\n",
    "\n",
    "# def gpt2_sentiment(text):\n",
    "#     if len(text.strip()) == 0:  # If the text is empty or just whitespace\n",
    "#         return 0.0  # Assign a neutral score or some default value\n",
    "    \n",
    "#     # Preprocess text\n",
    "#     inputs = preprocess_transformers([text], gpt2_tokenizer)\n",
    "    \n",
    "#     if inputs['input_ids'].shape[1] == 0:  # Check if input tensor is empty\n",
    "#         return 0.0\n",
    "#     # inputs = preprocess_transformers([text], gpt2_tokenizer)\n",
    "#     outputs = gpt2_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "#     positive_prob = torch.sigmoid(outputs).mean().item()\n",
    "#     return positive_prob\n",
    "\n",
    "\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# vader_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Vader Model \n",
    "# def vader_sentiment(text):\n",
    "#     return vader_analyzer.polarity_scores(text)['compound']\n",
    "\n",
    "# # TextBlob Model \n",
    "# def textblob_sentiment(text):\n",
    "#     return TextBlob(text).sentiment.polarity\n",
    "\n",
    "\n",
    "# from nltk import pos_tag\n",
    "\n",
    "# def pos_tags(text):\n",
    "#     tags = pos_tag(text.split())\n",
    "#     noun_count = sum(1 for word, tag in tags if tag.startswith('NN'))\n",
    "#     verb_count = sum(1 for word, tag in tags if tag.startswith('VB'))\n",
    "#     return noun_count, verb_count\n",
    "\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# # TF-IDF Method\n",
    "# tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "# X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# def tfidf_features(texts):\n",
    "#     return tfidf_vectorizer.transform(texts).toarray()\n",
    "\n",
    "\n",
    "# # SVM Model with TF-IDF features\n",
    "# svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(probability=True))\n",
    "# svm_model.fit(X_train_tfidf, Y_train)\n",
    "# svm_predictions = svm_model.predict_proba(X_test_tfidf)[:, 1]  # Probability of positive class\n",
    "\n",
    "# def svm_score(tfidf_features):\n",
    "#     svm_model.predict_proba(tfidf_features)[:, 1]\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "\n",
    "\n",
    "# # LSTM Model\n",
    "# def Build_lstm_model(input_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(MAX_NUM_WORDS, 128, input_length=input_length))\n",
    "#     model.add(LSTM(128, return_sequences=False))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def Build_cnn_model(input_length):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(input_dim=5000, output_dim=128, input_length=input_length))\n",
    "#     model.add(Conv1D(64, 5, activation='relu'))\n",
    "#     model.add(MaxPooling1D(pool_size=4))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # Tokenizer for LSTM/CNN\n",
    "# MAX_NUM_WORDS = 20000\n",
    "# MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "# # Create Tokenizer for LSTM and CNN\n",
    "# tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "# X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "# # LSTM Model training\n",
    "# lstm_model = Build_lstm_model(input_length=MAX_SEQUENCE_LENGTH)\n",
    "# lstm_model.fit(X_train_pad, np.array(Y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# # CNN Model training\n",
    "# cnn_model = Build_cnn_model(input_length=MAX_SEQUENCE_LENGTH)\n",
    "# cnn_model.fit(X_train_pad, np.array(Y_train), epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "\n",
    "# # LSTM and CNN Predictions\n",
    "# lstm_predictions = lstm_model.predict(X_test_pad).flatten()\n",
    "# cnn_predictions = cnn_model.predict(X_test_pad).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # nltk.download('averaged_perceptron_tagger')\n",
    "# # nltk.download('averaged_perceptron_tagger_eng')\n",
    "# def hybrid_model(texts, clean_text):\n",
    "#     results = []\n",
    "    \n",
    "#     for i, text in enumerate(texts):\n",
    "#         # Vader Sentiment\n",
    "#         vader_score = vader_sentiment(clean_text[i])\n",
    "#         textblob_score = textblob_sentiment(text)\n",
    "\n",
    "#         # Transformer Models (RoBERTa, BERT)\n",
    "#         roberta_score = roberta_sentiment(clean_text[i])\n",
    "#         bert_score = bert_sentiment(clean_text[i])\n",
    "#         gpt2_score = gpt2_sentiment(clean_text[i])\n",
    "\n",
    "#         # LSTM and CNN Predictions\n",
    "#         lstm_score = lstm_predictions[i]\n",
    "#         cnn_score = cnn_predictions[i]\n",
    "\n",
    "#         # SVM Prediction (using TF-IDF)\n",
    "#         tfidf_features = tfidf_vectorizer.transform([clean_text[i]])\n",
    "#         svm_score = svm_model.predict(tfidf_features)[0]\n",
    "\n",
    "\n",
    "\n",
    "#         # POS Tags (custom function for counting nouns and verbs)\n",
    "#         noun_count, verb_count = pos_tags(text)\n",
    "\n",
    "#         # Calculate average score (simple hybrid result aggregation)\n",
    "#         avg_score = np.mean([\n",
    "#             vader_score, textblob_score,  roberta_score, bert_score, gpt2_score, lstm_score, cnn_score, svm_score\n",
    "#         ])\n",
    "\n",
    "#         # VADER\n",
    "#         if vader_score > 0:\n",
    "#             sentiment_vader = \"Positive\"\n",
    "#         elif vader_score == 0:\n",
    "#             sentiment_vader = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_vader = \"Negative\"\n",
    "\n",
    "#         # TextBlob\n",
    "#         if textblob_score > 0:\n",
    "#             sentiment_textblob = \"Positive\"\n",
    "#         elif textblob_score == 0:\n",
    "#             sentiment_textblob = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_textblob = \"Negative\"\n",
    "\n",
    "#         # Gpt2\n",
    "#         if gpt2_score > 0.55:\n",
    "#             sentiment_gpt2 = \"Positive\"\n",
    "#         elif gpt2_score < 0.45:\n",
    "#             sentiment_gpt2 = \"Neutral\"\n",
    "#         else:\n",
    "#             sentiment_gpt2 = \"Negative\"\n",
    "\n",
    "#         # RoBERTa\n",
    "#         if roberta_score > 0.55:\n",
    "#             sentiment_roberta = \"Positive\"\n",
    "#         elif roberta_score < 0.45:\n",
    "#             sentiment_roberta = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_roberta = \"Neutral\"\n",
    "\n",
    "#         # BERT\n",
    "#         if bert_score > 0.55:\n",
    "#             sentiment_bert = \"Positive\"\n",
    "#         elif bert_score < 0.45:\n",
    "#             sentiment_bert = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_bert = \"Neutral\"\n",
    "\n",
    "#         # CNN\n",
    "#         if cnn_score > 0.55:\n",
    "#             sentiment_cnn = \"Positive\"\n",
    "#         elif cnn_score < 0.45:\n",
    "#             sentiment_cnn = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_cnn = \"Neutral\"\n",
    "\n",
    "#         # LSTM\n",
    "#         if lstm_score > 0.55:\n",
    "#             sentiment_lstm = \"Positive\"\n",
    "#         elif lstm_score < 0.45:\n",
    "#             sentiment_lstm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_lstm = \"Neutral\"\n",
    "        \n",
    "#         # SVM\n",
    "#         if svm_score == 1:\n",
    "#             sentiment_svm = \"Positive\"\n",
    "#         elif svm_score == 0:\n",
    "#             sentiment_svm = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_svm = \"Neutral\"\n",
    "\n",
    "#         # Average score\n",
    "#         if avg_score > 0.55:\n",
    "#             sentiment_avg = \"Positive\"\n",
    "#         elif avg_score < 0.45:\n",
    "#             sentiment_avg = \"Negative\"\n",
    "#         else:\n",
    "#             sentiment_avg = \"Neutral\"\n",
    "\n",
    "#         # Append results for this text\n",
    "#         results.append({\n",
    "#             'text': text,\n",
    "#             'clean_text': clean_text[i],\n",
    "#             'vader': vader_score,\n",
    "#             'textblob': textblob_score,\n",
    "#             'roberta': roberta_score,\n",
    "#             'bert': bert_score,\n",
    "#             'gpt2': gpt2_score,\n",
    "#             'cnn': cnn_score,\n",
    "#             'lstm': lstm_score,\n",
    "#             'svm': svm_score,\n",
    "#             'average_score': avg_score,\n",
    "#             'sentiment': df['sentiment'][i],\n",
    "#             'vader_sentiment': sentiment_vader,\n",
    "#             'textblob_sentiment': sentiment_textblob,\n",
    "#             'roberta_sentiment': sentiment_roberta,\n",
    "#             'bert_sentiment': sentiment_bert,\n",
    "#             'gpt2_sentiment': sentiment_gpt2,\n",
    "#             'cnn_sentiment': sentiment_cnn,\n",
    "#             'lstm_sentiment': sentiment_lstm,\n",
    "#             'svm_sentiment': sentiment_svm,\n",
    "#             'avg_sentiment': sentiment_avg,\n",
    "#             'nouncount_': noun_count,\n",
    "#             'verbcount_': verb_count\n",
    "#         })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "# def clean_texts(texts):\n",
    "#     return [str(text) if isinstance(text, str) else \"\" for text in texts]\n",
    "\n",
    "# df['review'] = df['review'].fillna('').astype(str)\n",
    "# min_length = min(len(X_text_test), len(X_test))\n",
    "\n",
    "# texts = X_text_test[:min_length].reset_index(drop=True)\n",
    "# texts = clean_texts(texts)\n",
    "# clean_text = X_test[:min_length].reset_index(drop=True)\n",
    "# # vader_texts = X_vader_test[:min_length].reset_index(drop=True)\n",
    "# # transformers_texts = X_transformers_test[:min_length].reset_index(drop=True)\n",
    "# # cnn_lstm_texts = X_cnn_lstm_test[:min_length].reset_index(drop=True)\n",
    "# # svm_texts = X_svm_test[:min_length].reset_index(drop=True)\n",
    "\n",
    "# # Run the hybrid model\n",
    "# hybrid_results = hybrid_model(texts, clean_text)\n",
    "\n",
    "# results_df = pd.DataFrame(hybrid_results)\n",
    "\n",
    "# results_df.to_csv('amazon_sentiment_result2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# from textblob import TextBlob\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from transformers import pipeline\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# # Instantiate pre-trained models\n",
    "# vader_analyzer = SentimentIntensityAnalyzer()  # VADER Model\n",
    "# roberta_pipeline = pipeline('sentiment-analysis', model='roberta-base')  # RoBERTa Model\n",
    "# bert_pipeline = pipeline('sentiment-analysis', model='bert-base-uncased')  # BERT Model\n",
    "\n",
    "# # SVM Model Setup (with TF-IDF)\n",
    "# svm_model = make_pipeline(TfidfVectorizer(max_features=5000), SVC(probability=True))\n",
    "# svm_model.fit(X_train_svm, y_train)  # Train the SVM model on your preprocessed data\n",
    "\n",
    "# # CNN-LSTM model for sentiment analysis\n",
    "# tokenizer = Tokenizer(num_words=5000)\n",
    "# tokenizer.fit_on_texts(X_train_cnn_lstm)\n",
    "# X_train_seq = tokenizer.texts_to_sequences(X_train_cnn_lstm)\n",
    "# X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
    "\n",
    "# cnn_lstm_model = Sequential()\n",
    "# cnn_lstm_model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))\n",
    "# cnn_lstm_model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "# cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "# cnn_lstm_model.add(LSTM(128))\n",
    "# cnn_lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# cnn_lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# cnn_lstm_model.fit(X_train_pad, y_train, epochs=5, batch_size=64)\n",
    "\n",
    "# # Function Definitions for each model\n",
    "# def vader_sentiment(text):\n",
    "#     score = vader_analyzer.polarity_scores(text)\n",
    "#     return score['compound']  # Returns a score between -1 and 1\n",
    "\n",
    "# def roberta_sentiment(text):\n",
    "#     result = roberta_pipeline(text)[0]\n",
    "#     return result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
    "\n",
    "# def bert_sentiment(text):\n",
    "#     result = bert_pipeline(text)[0]\n",
    "#     return result['score'] if result['label'] == 'POSITIVE' else -result['score']\n",
    "\n",
    "# def textblob_sentiment(text):\n",
    "#     return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# def svm_sentiment(text):\n",
    "#     probabilities = svm_model.predict_proba([text])[0]\n",
    "#     positive_prob = probabilities[1]\n",
    "#     negative_prob = probabilities[0]\n",
    "#     return (positive_prob - negative_prob)\n",
    "\n",
    "# def cnn_lstm_sentiment(text):\n",
    "#     text_seq = tokenizer.texts_to_sequences([text])\n",
    "#     text_pad = pad_sequences(text_seq, maxlen=100)\n",
    "#     score = cnn_lstm_model.predict(text_pad)[0][0]\n",
    "#     return score * 2 - 1  # Scale between -1 and 1\n",
    "\n",
    "# # Hybrid Model - Combining all sentiment results\n",
    "# def hybrid_sentiment(text):\n",
    "#     # Get scores from each model\n",
    "#     scores = [\n",
    "#         vader_sentiment(text),\n",
    "#         roberta_sentiment(text),\n",
    "#         bert_sentiment(text),\n",
    "#         textblob_sentiment(text),\n",
    "#         svm_sentiment(text),\n",
    "#         cnn_lstm_sentiment(text)\n",
    "#     ]\n",
    "#     # Averaging scores from all models\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# # Example usage of hybrid model\n",
    "# sample_text = \"I absolutely love this product! It exceeded my expectations.\"\n",
    "# hybrid_score = hybrid_sentiment(sample_text)\n",
    "# print(f\"Hybrid Sentiment Score for the text: {hybrid_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
